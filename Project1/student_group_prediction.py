# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_IgXIUZF0Z3TKWBC0pjf3BfqbUrEycM
"""



"""
1st project of Athanasios Koukosias Student ID(AM):2122160
Academic Year: 2024-2025
University of Thessaly
Class: Computetional Intelligence and Machine Learning
Professor: Dr. Kolomvatsos Konstantinos
Professor Assistants(Lab Professors): Fountas Panagiotis-Papathanasaki Maria
Data:https://archive.ics.uci.edu/dataset/320/student+performance
"""

# Importing all the important libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split



# Creating the main set up of the program
dataset = pd.read_csv('student-por.csv',delimiter=';')
dataset = dataset.drop(["school","address","Pstatus","reason","traveltime","paid","higher","famrel","nursery","Dalc","Walc"],axis=1)
SEED = 42 #This is the seed that we are going to use in some parts of our program. If we want to change the output, simply change the seed that is mainly used in random functions

"""
In order to make the classification I decided not to rely directly on K-Means or any other pre-existing
clustering algorithm. Instead I created this tailored function that takes into
consideration only the numerical features like G1,G2,G3, Failures, Absences, Studytime.
Then, by adding some weights to my function I distribute virtual coordinates to each entity/student. Thus, creating a more sofisticated approach.
Finally, on these virtual coordinates I can apply the K-Means algorithm that was mentioned in class:)
"""

def virtual_coordinates_calculator(data,category):
  selected_features = data[['G1', 'G2', 'G3', 'failures', 'absences','studytime','freetime','goout']]
  # Standardize the features to have mean=0 and variance=1
  scaler = StandardScaler()    
  scaled_features = scaler.fit_transform(selected_features)
  # Calculate virtual coordinates based on feature mappings
  # Example: X-axis uses school statistics, Y-axis uses out of school time management
  # Adjust weights as needed
  w_G1 = 0.23
  w_G2 = 0.23
  w_G3 = 0.23
  w_failures = 0.2
  w_absences = 0.2
  w_studytime = 0.5
  w_freetime = 0.3
  w_goout = 0.2
  
  if category == 'new': # In the new student case we have to handle 1-D data so our function needs to be customized appropriently
      scaled_features = scaled_features[-1]
      X = w_G1 * scaled_features[0] + w_G2 * scaled_features[1] + w_G3 * scaled_features[2] + w_failures * scaled_features[3] + w_absences * scaled_features[4] # School Stats
      Y = w_studytime * scaled_features[5] + w_freetime * scaled_features[6] + w_goout * scaled_features[7]  # Out of School Stats
      
  else:
      X = w_G1 * scaled_features[:, 0] + w_G2 * scaled_features[:, 1] + w_G3 * scaled_features[:, 2] + w_failures * scaled_features[:,3] + w_absences * scaled_features[:,4] # School Stats
      Y = w_studytime * scaled_features[:, 5] + w_freetime * scaled_features[:, 6] + w_goout * scaled_features[:,7]  # Out of School Stats
  return X,Y

def classifier(dataset):
  #Stack the virtual coordinates in a list
  X,Y = virtual_coordinates_calculator(dataset,"train") # create the virtual coordinates
  virtual_coordinates = np.column_stack((X,Y)) # stack the 1D arrays as columns into a 2D array
  #print(virtual_coordinates)

  #Get the cluster number by the user
  flag = 0
  while flag==0:
    try:
      cluster_number = int(input("Please provide the number of student categories you want to create-->"))
      if cluster_number > 1:
        flag = 1
    except:
      print("Categories number must be a number between 1-7(any number above that may cause problems in the programs normal operation!)")

  # Apply K-means clustering
  kmeans = KMeans(n_clusters=cluster_number, random_state=SEED)
  labels = kmeans.fit_predict(virtual_coordinates)
  # Add cluster labels to the original data
  dataset['Cluster'] = labels
  return cluster_number,X,Y,dataset,kmeans

cluster_number,X,Y,dataset,kmeans = classifier(dataset)

def plot_clusters(X,Y,labels):
  # Plot the clusters (for 2D, only use X and Y; for 3D, add Z)
  fig = plt.figure(figsize=(8, 6))
  ax = fig.add_subplot()
  ax.scatter(X, Y, c=labels)
  plt.title("Student Clusters in Virtual Coordinate Space")
  ax.set_xlabel("X (School Related Stats)")
  ax.set_ylabel("Y (Out of School Stats)")
  plt.show()

plot_clusters(X,Y,dataset['Cluster'])

# Function to plot categorical columns
def plot_categorical(data, column, color):
    counts = data[column].value_counts().sort_index()
    plt.bar(counts.index, counts.values, color=color)
    plt.title(f"Distribution of Students by {column}")
    plt.xlabel(column)
    plt.ylabel("Number of Students")
    plt.show()

# Function to plot numerical columns
def plot_numerical(data, column, color):
    counts = data[column].value_counts().sort_index()  # Count occurrences and sort by value
    plt.bar(counts.index, counts.values, color=color)
    plt.title(f"Distribution of Students by {column}")
    plt.xlabel(column)
    plt.ylabel("Number of Students")
    plt.xticks(counts.index)
    plt.show()

# Group Statistics Function
def statistics_of_cluster(group,color,statistics):
  df = dataset[dataset['Cluster']==group] # We extract the data of the students in the specific group
  # Plot categorical columns
  #plot_categorical(df, 'school', color)
  plot_categorical(df, 'sex', color)
  #plot_categorical(df, 'address', color)
  plot_categorical(df, 'famsize', color)
  #plot_categorical(df, 'Pstatus', color)
  plot_categorical(df, 'Mjob', color)
  plot_categorical(df, 'Fjob', color)
  #plot_categorical(df, 'reason', color)
  plot_categorical(df, 'guardian', color)
  plot_categorical(df, 'schoolsup', color)
  plot_categorical(df, 'famsup', color)
  #plot_categorical(df, 'paid', color)
  plot_categorical(df, 'activities', color)
  #plot_categorical(df, 'nursery', color)
  #plot_categorical(df, 'higher', color)
  plot_categorical(df, 'internet', color)
  plot_categorical(df, 'romantic', color)

  # Plot numerical columns
  
  #plot_numerical(df, 'traveltime', color)
  plot_numerical(df, 'studytime', color)
  plot_numerical(df, 'failures', color)
  #plot_numerical(df, 'famrel', color)
  plot_numerical(df, 'freetime', color)
  plot_numerical(df, 'goout', color)
  #plot_numerical(df, 'Dalc', color)
  #plot_numerical(df, 'Walc', color)
  plot_numerical(df, 'health', color)
  plot_numerical(df, 'absences', color)
  plot_numerical(df, 'G1', color)
  plot_numerical(df, 'G2', color)
  plot_numerical(df, 'G3', color)
  print(df.describe())
  statistics.append(df.describe())

# List of colors to use for each cluster
colors = ['skyblue', 'salmon', 'lightgreen', 'orange', 'purple','red','blue','lime','grey']
statistics = [] # This function will hold the basic statistics for all the subgroups
for group in range(cluster_number): # We iterate through all groups defined by K-Means
  color = colors[group % len(colors)]   # Color selection based on the index modulo the length of the colors list
  statistics_of_cluster(group,color,statistics)

# New student function
new_entry_help = {
    "sex" : "M or F",
    "age" : "integer under 20",
    "famsize" : "GT3 or LE3 meaning Greater Than 3 or Less-Equal to 3",
    "Medu" : "integer between 0 and 5",
    "Fedu" : "integer between 0 and 5",
    "Mjob" : "Mothers Job (e.g. at_home/health/services/teacher/other)",
    "Fjob" : "Fathers Job (e.g. at_home/health/services/teacher/other)",
    "guardian" : "mother or father",
    "studytime": "How many hours per day does the student study?",
    "failures" : "How many failures did the student have during the current period?",
    "schoolsup" : "Does the student recieve support from school? e.g. parallel support teacher: yes/no",
    "famsup" : "Does the student recieve help from others at home? yes/no",
    "activities" : "Does the student attend any out of school activities? yes/no",
    "internet" : "Does the student have access to the internet? yes/no",
    "romantic" : "Does the student have a romantic relationship? yes/no",
    "freetime" : "How many hours of free time does the student have in a day? int between 0 and 5",
    "goout" : "How many times per week does the student go out? int between 0 and 5",
    "health" : "How good is the students health condition? int between 0 and 5",
    "absences" : "How many absences did the student have during the current period? int",
    "G1" : "What was the students grade during the 1st Semester? int between 0 and 20",
    "G2" : "What was the students grade during the 1st Semester? int between 0 and 20",
    "G3" : "What was the students grade during the 1st Semester? int between 0 and 20"
}


def new_student(columns,guide):
  new_entry = [] # This is where all the new info will be stored
  for attribute in columns:
    new_entry.append(input(f"Add info for {attribute}(INFO:{guide[attribute]}) -->")) # Dynamic message for dislaying input info
  return new_entry


# Turning the output into a dictionary and then into a dataframe in order to be compatible with the function I wrote
columns = ['sex','age','famsize','Medu','Fedu','Mjob','Fjob','guardian','studytime','failures','schoolsup','famsup','activities','internet','romantic','freetime','goout','health','absences','G1','G2','G3']


# Add a new student and predict where they belong
new_entry = new_student(columns,new_entry_help) # I dorp the cluster column since its the one I want to predict later



new_student_df = pd.DataFrame([new_entry], columns=columns)
new_student_df=pd.concat([dataset,new_student_df])

#print(new_student_df)
newX,newY = virtual_coordinates_calculator(new_student_df,"new")
new_student_coordinates = np.column_stack((newX,newY))
where_belongs = kmeans.predict(new_student_coordinates) # needs to be a dictionary not array or list
where_belongs = where_belongs[0]
new_student_df.at[-1,'Cluster'] = where_belongs
print(f"The new student entry belongs in group no.{new_student_df.at[0,'Cluster']}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
def knn_train(data):
  X = data.drop('Cluster', axis=1)
  y = data['Cluster']
  # Split the data into training and test sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  # Scale the features using StandardScaler
  scaler = StandardScaler()
  X_train = scaler.fit_transform(X_train)
  X_test = scaler.transform(X_test)
  knn = KNeighborsClassifier(n_neighbors=7)
  knn.fit(X_train, y_train)
  y_pred = knn.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  print("Accuracy:", accuracy)
  return knn,scaler

tempDF=pd.DataFrame({'X':X,'Y':Y,'Cluster':dataset['Cluster']}) # I create a df for the knn algorithm training
#ths is so that I combine the virtual coordinates of the original dataset with the occured group labels
#will do the same thing for the new student
#print(tempDF)
knn,scaler = knn_train(tempDF)
# For the new student
newStempDF = pd.DataFrame({'X': [newX.item()], 'Y': [newY.item()]}) # Create a DataFrame with only 'X' and 'Y'
distances, indices = knn.kneighbors(newStempDF, n_neighbors=3)
neighbors = dataset.iloc[indices[0]]
print(f"Neighbors: {neighbors}")

# Plot the results
plt.figure(figsize=(8, 6))  # Adjust figure size as needed

# Scatter plot of existing data points with cluster colors
for cluster in tempDF['Cluster'].unique():
    plt.scatter(tempDF[tempDF['Cluster'] == cluster]['X'],
                tempDF[tempDF['Cluster'] == cluster]['Y'],
                label=f'Cluster {cluster}')

# Plot the new student's data point as well
plt.scatter(newStempDF['X'], newStempDF['Y'], color='black', marker='*', s=100, label='New Student')

plt.title('KNN Neighbors and Clusters')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()
